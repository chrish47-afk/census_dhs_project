{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "833fd787-3a89-4647-849a-fc4ef8fd75f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">import spark.implicits._\n",
       "import org.apache.spark.sql.SparkSession\n",
       "startTime: Long = 2944849751000\n",
       "filepath_census: String = abfss://chrish47@bigdatastorage24.dfs.core.windows.net/Project_Data_Census_DHS/acs5_immigration_foreign_allyears_final.csv\n",
       "filepath_dhs2: String = abfss://chrish47@bigdatastorage24.dfs.core.windows.net/Project_Data_Census_DHS/DHS_table2_lawful_permanent_resident.csv\n",
       "census: org.apache.spark.sql.DataFrame = [year: string, state_fips: string ... 385 more fields]\n",
       "dhslawful: org.apache.spark.sql.DataFrame = [Region and country of last residence: string, 2014: string ... 9 more fields]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">import spark.implicits._\nimport org.apache.spark.sql.SparkSession\nstartTime: Long = 2944849751000\nfilepath_census: String = abfss://chrish47@bigdatastorage24.dfs.core.windows.net/Project_Data_Census_DHS/acs5_immigration_foreign_allyears_final.csv\nfilepath_dhs2: String = abfss://chrish47@bigdatastorage24.dfs.core.windows.net/Project_Data_Census_DHS/DHS_table2_lawful_permanent_resident.csv\ncensus: org.apache.spark.sql.DataFrame = [year: string, state_fips: string ... 385 more fields]\ndhslawful: org.apache.spark.sql.DataFrame = [Region and country of last residence: string, 2014: string ... 9 more fields]\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "//////////////////////////////////\n",
    "//////// Sourcing in Data ////////\n",
    "//////////////////////////////////\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "//////// Measure Start Time ////////\n",
    "val startTime = System.nanoTime() //nano is more precised than milli.\n",
    "\n",
    "// Reading in Data Files\n",
    "val filepath_census = abfssBasePath + \"Project_Data_Census_DHS/acs5_immigration_foreign_allyears_final.csv\"\n",
    "val filepath_dhs2 = abfssBasePath + \"Project_Data_Census_DHS/DHS_table2_lawful_permanent_resident.csv\"\n",
    "\n",
    "val census = spark.read\n",
    "                  .option(\"header\", \"true\") // Use the first row as column names\n",
    "                  .csv(filepath_census)\n",
    "val dhslawful = spark.read\n",
    "                .option(\"header\", \"true\")\n",
    "                .csv(filepath_dhs2)\n",
    "//census.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1a4f74-8451-4d33-8ef0-701cab81a731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Number of census df columns: 387\n",
       "Number of census df rows: 352546\n",
       "Census df - Number of unique States: 50\n",
       "Census df - Number of unique Counties: 3156\n",
       "Census df - Number of unique column variables that end with E: 210\n",
       "Census df - Number of unique column variables that end with M: 165\n",
       "Unique data types in census df:\n",
       "StringType\n",
       "Unique data types in dhslawful df:\n",
       "StringType\n",
       "Code execution took 10.10300368 seconds.\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.SparkSession\n",
       "startTime: Long = 2962443277712\n",
       "numColumns: Int = 387\n",
       "numRows: Long = 352546\n",
       "uniquestatename: Long = 50\n",
       "uniquecountyname: Long = 3156\n",
       "uniquevariableE: Int = 210\n",
       "uniquevariableM: Int = 165\n",
       "datatypescensus: Array[org.apache.spark.sql.types.DataType] = Array(StringType)\n",
       "datatypeslawful: Array[org.apache.spark.sql.types.DataType] = Array(StringType)\n",
       "endTime: Long = 2972546281392\n",
       "duration: Double = 10.10300368\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Number of census df columns: 387\nNumber of census df rows: 352546\nCensus df - Number of unique States: 50\nCensus df - Number of unique Counties: 3156\nCensus df - Number of unique column variables that end with E: 210\nCensus df - Number of unique column variables that end with M: 165\nUnique data types in census df:\nStringType\nUnique data types in dhslawful df:\nStringType\nCode execution took 10.10300368 seconds.\nimport spark.implicits._\nimport org.apache.spark.sql.SparkSession\nstartTime: Long = 2962443277712\nnumColumns: Int = 387\nnumRows: Long = 352546\nuniquestatename: Long = 50\nuniquecountyname: Long = 3156\nuniquevariableE: Int = 210\nuniquevariableM: Int = 165\ndatatypescensus: Array[org.apache.spark.sql.types.DataType] = Array(StringType)\ndatatypeslawful: Array[org.apache.spark.sql.types.DataType] = Array(StringType)\nendTime: Long = 2972546281392\nduration: Double = 10.10300368\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "//////////////////////////////////\n",
    "//// Data Review - Validation ////\n",
    "//////////////////////////////////\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "//////// Measure Start Time ////////\n",
    "val startTime = System.nanoTime() //nano is more precised than milli.\n",
    "\n",
    "//////// Unique Conditions - Validation ////////\n",
    "// Number of columns\n",
    "val numColumns = census.columns.length\n",
    "println(s\"Number of census df columns: $numColumns\")\n",
    "\n",
    "// Number of rows\n",
    "val numRows = census.count()\n",
    "println(s\"Number of census df rows: $numRows\")\n",
    "\n",
    "// Show unique values of a specific column (e.g., \"name\")\n",
    "val uniquestatename = census.select(\"state_name\") //This should equal 50 states, 50 US states.\n",
    "                          .distinct()\n",
    "                          .count()\n",
    "val uniquecountyname = census.select(\"county_name\")\n",
    "                              .distinct()\n",
    "                              .count()\n",
    "val uniquevariableE = census.columns.filter(_.endsWith(\"E\"))\n",
    "                                    .length\n",
    "val uniquevariableM = census.columns.filter(_.endsWith(\"M\"))\n",
    "                                    .length\n",
    "\n",
    "println(s\"Census df - Number of unique States: $uniquestatename\")\n",
    "println(s\"Census df - Number of unique Counties: $uniquecountyname\")\n",
    "println(s\"Census df - Number of unique column variables that end with E: $uniquevariableE\")\n",
    "println(s\"Census df - Number of unique column variables that end with M: $uniquevariableM\")\n",
    "//uniquestatename.show()\n",
    "\n",
    "//////// Schema - Data Types ////////\n",
    "// Get the schema of the DataFrame\n",
    "val datatypescensus = census.schema.fields.map(_.dataType)\n",
    "                                    .distinct\n",
    "val datatypeslawful = dhslawful.schema.fields.map(_.dataType)\n",
    "                                    .distinct\n",
    "\n",
    "// Print the unique data types\n",
    "println(\"Unique data types in census df:\")\n",
    "datatypescensus.foreach(println)\n",
    "println(\"Unique data types in dhslawful df:\")\n",
    "datatypeslawful.foreach(println)\n",
    "\n",
    "//////// Measure End Time ////////\n",
    "val endTime = System.nanoTime()\n",
    "\n",
    "// Calculate and print execution time in seconds\n",
    "val duration = (endTime - startTime) / 1e9d\n",
    "println(s\"Code execution took $duration seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a88079-e613-435a-aa42-5e157a27bf55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Total US Population in 2022: 3.30427006E8\n",
       "Total Population for Washington in 2022: 7688549.0\n",
       "Total Foreign Population for Washington in 2022: 7688549.0\n",
       "Total Population for Washington, Clark County in 2022: 2254371.0\n",
       "Persons obtaining lawful permanent resident status by region and selected country of last residence: 133,110\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.functions._\n",
       "censuspop: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [year: string, state_name: string ... 3 more fields]\n",
       "totalcensuspop: Any = 3.30427006E8\n",
       "totalpop: Any = 7688549.0\n",
       "foreignpop: Any = 7688549.0\n",
       "totalpop_bycounty: Any = 2254371.0\n",
       "dhslawfula: Any = 133,110\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Total US Population in 2022: 3.30427006E8\nTotal Population for Washington in 2022: 7688549.0\nTotal Foreign Population for Washington in 2022: 7688549.0\nTotal Population for Washington, Clark County in 2022: 2254371.0\nPersons obtaining lawful permanent resident status by region and selected country of last residence: 133,110\nimport spark.implicits._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\ncensuspop: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [year: string, state_name: string ... 3 more fields]\ntotalcensuspop: Any = 3.30427006E8\ntotalpop: Any = 7688549.0\nforeignpop: Any = 7688549.0\ntotalpop_bycounty: Any = 2254371.0\ndhslawfula: Any = 133,110\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "/////////////////////////////////\n",
    "//// Data Questions - Manual ////\n",
    "/////////////////////////////////\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._ // For aggregation functions\n",
    "\n",
    "val censuspop = census.select(\n",
    "                              $\"year\",\n",
    "                              $\"state_name\",\n",
    "                              $\"county_name\",\n",
    "                              $\"B05001_001E\",\n",
    "                              $\"B05002_001E\"\n",
    "                              )\n",
    "                    .filter($\"year\" === 2022)\n",
    "                    .filter($\"state_name\" === \"Washington\")\n",
    "////////Census - Calculating the sum of B05001_001E at different levels////////\n",
    "val totalcensuspop = census.filter($\"year\" === 2022)\n",
    "                          .agg(sum($\"B05001_001E\").alias(\"total_us_population\"))\n",
    "                          .collect()(0)(0)\n",
    "val totalpop = censuspop.agg(sum($\"B05001_001E\").alias(\"total_state_population\"))\n",
    "                        .collect()(0)(0) // Collecting the sum as a scalar value\n",
    "val foreignpop = censuspop.agg(sum($\"B05002_001E\").alias(\"total_state_foreign_population\"))\n",
    "                        .collect()(0)(0) // Collecting the sum as a scalar value\n",
    "val totalpop_bycounty = censuspop.filter($\"county_name\".contains(\"King County\"))\n",
    "                                .groupBy($\"county_name\")\n",
    "                                .agg(sum($\"B05001_001E\").alias(\"total_county_population\"))\n",
    "                                .collect()(0)(1) // Collecting the sum as a scalar value\n",
    "\n",
    "println(s\"Total US Population in 2022: $totalcensuspop\")\n",
    "println(s\"Total Population for Washington in 2022: $totalpop\")\n",
    "println(s\"Total Foreign Population for Washington in 2022: $foreignpop\")\n",
    "println(s\"Total Population for Washington, Clark County in 2022: $totalpop_bycounty\")\n",
    "\n",
    "////////DHS - Wrangling for different level and country level data & agggregating as necessary////////\n",
    "/**\n",
    "Some of the DHS data just doesn't make sense to me. Including the exclusion of some countries, in particularly central and southern american countries.\n",
    "For that areason. Only Table 13, 17, and 19 are explored. There are other tables that can be further explored, but due to time limitations, I focused on the refugee section.\n",
    "**/\n",
    "val dhslawfula = dhslawful.filter($\"Region and country of last residence\" === \"Mexico\")\n",
    "                          .select($\"2014\")\n",
    "                          .collect()(0)(0)\n",
    "\n",
    "println(s\"Persons obtaining lawful permanent resident status by region and selected country of last residence: $dhslawfula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "06e59bd0-050c-4b61-b0a5-d50ae5cde94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "// NOTES\n",
    "/////////////////////////////////////////////\n",
    "// DataFrame and Collection Operations\n",
    "/////////////////////////////////////////////\n",
    "\n",
    "// .groupBy\n",
    "// - Groups rows in a DataFrame based on the values of one or more columns.\n",
    "// - Returns a RelationalGroupedDataset, allowing for aggregation operations on grouped data.\n",
    "// Example:\n",
    "// df.groupBy(\"state_name\").count()\n",
    "\n",
    "// .agg\n",
    "// - Performs aggregation functions (e.g., sum, count, avg) on grouped data from .groupBy.\n",
    "// - Returns a new DataFrame with aggregated results.\n",
    "// Example:\n",
    "// groupedData.agg(sum(\"population\").alias(\"total_population\"))\n",
    "\n",
    "// .collect\n",
    "// - Retrieves all rows of a DataFrame or Dataset to the driver node as an array.\n",
    "// - Should be used cautiously with large datasets due to memory limitations.\n",
    "// Example:\n",
    "// val rows = df.collect() // Returns an Array[Row]\n",
    "\n",
    "// .map\n",
    "// - Transforms elements in a collection or Dataset based on a specified function.\n",
    "// Example:\n",
    "// val doubled = Seq(1, 2, 3).map(x => x * 2) // Output: Seq(2, 4, 6)\n",
    "\n",
    "// .headOption\n",
    "// - Returns an Option containing the first element of a collection (if it exists) or None if the collection is empty.\n",
    "// Example:\n",
    "// val firstElement = Seq(1, 2, 3).headOption // Output: Some(1)\n",
    "// val emptyElement = Seq().headOption // Output: None\n",
    "\n",
    "// .getValuesMap\n",
    "// - Converts specific columns of a Row into a Map of column names and their corresponding values.\n",
    "// Example:\n",
    "// val row = Row(\"California\", 39538223)\n",
    "// row.getValuesMap(Seq(\"state\", \"population\"))\n",
    "// // Output: Map(\"state\" -> \"California\", \"population\" -> 39538223)\n",
    "\n",
    "/////////////////////////////////////////////\n",
    "// Scala Data Structures\n",
    "/////////////////////////////////////////////\n",
    "\n",
    "// Option[]\n",
    "// - A container that may or may not hold a value. Used to handle optional or missing values without null.\n",
    "// - Subtypes:\n",
    "//   - Some(value): Contains a value.\n",
    "//   - None: Represents the absence of a value.\n",
    "// Example:\n",
    "// val opt: Option[Int] = Some(5)\n",
    "// opt.getOrElse(0) // Output: 5\n",
    "\n",
    "// Some()\n",
    "// - Represents a value inside an Option.\n",
    "// Example:\n",
    "// val someValue = Some(42) // Output: Option[Int] = Some(42)\n",
    "\n",
    "// Seq\n",
    "// - A general-purpose, ordered collection in Scala (immutable by default).\n",
    "// Example:\n",
    "// val numbers = Seq(1, 2, 3) // Output: Seq[Int] = List(1, 2, 3)\n",
    "\n",
    "// Map[]\n",
    "// - A collection of key-value pairs, where each key maps to a value.\n",
    "// Example:\n",
    "// val statePopulations = Map(\"California\" -> 39538223, \"Texas\" -> 29145505)\n",
    "// statePopulations(\"California\") // Output: 39538223\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f3d25b3-6edf-4ce1-862d-797220da4f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">US-level aggregation: Map(B05001_001E_total -&gt; 3.30427006E8, B05001_006E_total -&gt; 2.1568867E7)\n",
       "State-level aggregation: Map(B05001_001E_total -&gt; 7688549.0, B05001_006E_total -&gt; 590748.0)\n",
       "County-level aggregation: Map(county_name -&gt; King County, Washington, B05001_001E_total -&gt; 2254371.0, B05001_006E_total -&gt; 291945.0)\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
       "import org.apache.spark.sql.functions._\n",
       "aggregateCensusData: (census: org.apache.spark.sql.DataFrame, year: Int, stateName: Option[String], countyName: Option[String], variables: Seq[String])Map[String,Any]\n",
       "resultUS: Map[String,Any] = Map(B05001_001E_total -&gt; 3.30427006E8, B05001_006E_total -&gt; 2.1568867E7)\n",
       "resultState: Map[String,Any] = Map(B05001_001E_total -&gt; 7688549.0, B05001_006E_total -&gt; 590748.0)\n",
       "resultCounty: Map[String,Any] = Map(county_name -&gt; King County, Washington, B05001_001E_total -&gt; 2254371.0, B05001_006E_total -&gt; 291945.0)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">US-level aggregation: Map(B05001_001E_total -&gt; 3.30427006E8, B05001_006E_total -&gt; 2.1568867E7)\nState-level aggregation: Map(B05001_001E_total -&gt; 7688549.0, B05001_006E_total -&gt; 590748.0)\nCounty-level aggregation: Map(county_name -&gt; King County, Washington, B05001_001E_total -&gt; 2254371.0, B05001_006E_total -&gt; 291945.0)\nimport spark.implicits._\nimport org.apache.spark.sql.{SparkSession, DataFrame}\nimport org.apache.spark.sql.functions._\naggregateCensusData: (census: org.apache.spark.sql.DataFrame, year: Int, stateName: Option[String], countyName: Option[String], variables: Seq[String])Map[String,Any]\nresultUS: Map[String,Any] = Map(B05001_001E_total -&gt; 3.30427006E8, B05001_006E_total -&gt; 2.1568867E7)\nresultState: Map[String,Any] = Map(B05001_001E_total -&gt; 7688549.0, B05001_006E_total -&gt; 590748.0)\nresultCounty: Map[String,Any] = Map(county_name -&gt; King County, Washington, B05001_001E_total -&gt; 2254371.0, B05001_006E_total -&gt; 291945.0)\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "////////////////////////////////////////\n",
    "//// Data Questions - Main Function ////\n",
    "////////////////////////////////////////\n",
    "/*\n",
    "aggregateCensusData is designed to aggregate census data at different geographic levels(US, State, county) based on user-specified variables.\n",
    "\n",
    "* conditional statements(if-else) vs. pattern matching\n",
    "Structure - Pattern Matching: Testing a value against multiple patterns. Top to bottom.:\n",
    "value match {\n",
    "  case pattern1 => // Code for pattern1\n",
    "  case pattern2 => // Code for pattern2\n",
    "  case _        => // Code for a default case\n",
    "}\n",
    "*/\n",
    "\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._ // For aggregation functions\n",
    "\n",
    "//////// Master Function ////////\n",
    "def aggregateCensusData(census: DataFrame, \n",
    "                        year: Int, \n",
    "                        stateName: Option[String] = None, // Optional\n",
    "                        countyName: Option[String] = None, // Optional\n",
    "                        variables: Seq[String]): Map[String, Any] = {\n",
    "\n",
    "  // Select relevant columns dynamically based on input variables\n",
    "  val selectedColumns = Seq($\"year\", $\"state_name\", $\"county_name\") ++ variables.map(census.col)\n",
    "  //Seq($\"year\", $\"state_name\", $\"county_name\"): A sequence of pre-defined column names.\n",
    "  //variables.map(census.col): Maps the variables (a sequence of column names as strings) to actual column objects in the census DataFrame.\n",
    "  val filteredCensus = census.select(selectedColumns: _*)\n",
    "                             .filter($\"year\" === year)\n",
    "\n",
    "  // Handle aggregation logic based on stateName and countyName inputs\n",
    "  val result = (stateName, countyName) match {\n",
    "    case (Some(state), Some(county)) => // Both stateName and countyName are provided. Aggregates data at the county level.\n",
    "      // Aggregate at the county level\n",
    "      val grouped = filteredCensus.filter($\"state_name\" === state && $\"county_name\".contains(county))\n",
    "                                  .groupBy($\"county_name\")\n",
    "      \n",
    "      val aggExprs = variables.map(v => sum(census.col(v)).alias(s\"${v}_total\"))\n",
    "      grouped.agg(aggExprs.head, aggExprs.tail: _*) //note: _* Expands a collection into a sequence of arguments.\n",
    "             .collect()\n",
    "             .map(row => row.getValuesMap[Any](row.schema.fieldNames))\n",
    "             .headOption.getOrElse(Map(\"Error\" -> \"No data found for the specified county\"))\n",
    "\n",
    "    case (Some(state), None) => // Only stateName is provided. Aggregates data at the state level.\n",
    "      // Aggregate at the state level\n",
    "      val stateFiltered = filteredCensus.filter($\"state_name\" === state)\n",
    "      val aggExprs = variables.map(v => sum(census.col(v)).alias(s\"${v}_total\"))\n",
    "      stateFiltered.agg(aggExprs.head, aggExprs.tail: _*)\n",
    "                   .collect()(0)\n",
    "                   .getValuesMap[Any](variables.map(v => s\"${v}_total\")) //I can also get rid of this line. Review.\n",
    "\n",
    "    case (None, None) => // No stateName or countyName is provided. Aggregates data at the national (US) level.\n",
    "      // Aggregate at the national (US) level\n",
    "      val aggExprs = variables.map(v => sum(census.col(v)).alias(s\"${v}_total\"))\n",
    "      filteredCensus.agg(aggExprs.head, aggExprs.tail: _*)\n",
    "                    .collect()(0)\n",
    "                    .getValuesMap[Any](variables.map(v => s\"${v}_total\")) //I can also get rid of this line. Review.\n",
    "\n",
    "    case _ => //Case 4: Return an error map indicating an invalid input.\n",
    "      Map(\"Error\" -> \"Invalid input: Provide either a state name or none for US-level aggregation.\")\n",
    "  }\n",
    "\n",
    "  result\n",
    "  // The function returns a Map[String, Any] containing the aggregated values for the requested geo level or an error.\n",
    "}\n",
    "\n",
    "//////// Example Calls ////////\n",
    "val resultUS = aggregateCensusData(\n",
    "  census = census,\n",
    "  year = 2022,\n",
    "  variables = Seq(\"B05001_001E\",\"B05001_006E\")\n",
    ")\n",
    "println(s\"US-level aggregation: $resultUS\")\n",
    "\n",
    "val resultState = aggregateCensusData(\n",
    "  census = census,\n",
    "  year = 2022,\n",
    "  stateName = Some(\"Washington\"),\n",
    "  variables = Seq(\"B05001_001E\",\"B05001_006E\")\n",
    ")\n",
    "println(s\"State-level aggregation: $resultState\")\n",
    "\n",
    "val resultCounty = aggregateCensusData(\n",
    "  census = census,\n",
    "  year = 2022,\n",
    "  stateName = Some(\"Washington\"),\n",
    "  countyName = Some(\"King County\"),\n",
    "  variables = Seq(\"B05001_001E\",\"B05001_006E\")\n",
    ")\n",
    "println(s\"County-level aggregation: $resultCounty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213f53dc-7a3e-44d7-8edc-3c4830369a19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Persons obtaining lawful permanent resident status by region or selected country of last residence: Map(2022 -&gt; 1,018,350)\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
       "import org.apache.spark.sql.functions._\n",
       "getLawfulResidentCount: (dhs: org.apache.spark.sql.DataFrame, year: String, country: Option[String])Any\n",
       "resultMexico: Any = Map(2022 -&gt; 1,018,350)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Persons obtaining lawful permanent resident status by region or selected country of last residence: Map(2022 -&gt; 1,018,350)\nimport spark.implicits._\nimport org.apache.spark.sql.{SparkSession, DataFrame}\nimport org.apache.spark.sql.functions._\ngetLawfulResidentCount: (dhs: org.apache.spark.sql.DataFrame, year: String, country: Option[String])Any\nresultMexico: Any = Map(2022 -&gt; 1,018,350)\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._ // For aggregation functions\n",
    "\n",
    "//////// Function for Lawful Resident Count ////////\n",
    "def getLawfulResidentCount(dhs: DataFrame, \n",
    "                           year: String, \n",
    "                           country: Option[String] = None): Any = {\n",
    "\n",
    "  // Select relevant columns dynamically (Region/Country and Year)\n",
    "  val selectedColumns = Seq($\"Region and country of last residence\", dhs.col(year))\n",
    "  val filteredDHS = dhs.select(selectedColumns: _*)\n",
    "\n",
    "  // Handle logic based on country input\n",
    "  val result = country match {\n",
    "    case Some(countryName) =>\n",
    "      // Get the count for a specific country\n",
    "      filteredDHS.filter($\"Region and country of last residence\" === countryName)\n",
    "                 .select(year)\n",
    "                 .collect()(0)\n",
    "                 .getValuesMap[Any](Seq(year)) //I can also get rid of this line. Review.\n",
    "    case None =>\n",
    "      // Default to \"Global\" or return a message\n",
    "      s\"Please provide a country name to retrieve the count.\"\n",
    "  }\n",
    "\n",
    "  result\n",
    "}\n",
    "\n",
    "// Example 1: Get count for a specific country and year\n",
    "val resultMexico = getLawfulResidentCount(\n",
    "  dhs = dhslawful, \n",
    "  year = \"2022\", \n",
    "  country = Some(\"Total\") //Total is also another option.\n",
    ")\n",
    "println(s\"Persons obtaining lawful permanent resident status by region or selected country of last residence: $resultMexico\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Census_DHS_Project 2024-12-12 00:42:24",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
